{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5c3e7bd-f182-421a-8e76-b8e009ec5158",
   "metadata": {},
   "source": [
    "## Install further packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30efd0c6-11d1-4266-a2fa-554760b25beb",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a038d13-a0ca-4140-94bd-c68606cce11c",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea9e155-c253-44d6-8954-7d5a397728eb",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "In the the `baseline_method` and `split` variables and some arguments in the `command`-string need to be changed, depending on the prediction, that is being evaluated:\n",
    "\n",
    "In 'baseline_method' set the names of all predictions you want to evaluate leaving out split and the word \"predictions\", e.g. if your predictions are called \"baseline_dev_predictions.jsonl\" just insert \"baseline\".\n",
    "\n",
    "In `split` insert the split-sets for which you want to evaluate predictions (can be \"train\", \"dev\" or \"test\").\n",
    "\n",
    "In the `command` string:\n",
    "\n",
    "For `--data` set the name of the name of the folder that contains the data set on which you want to evaluate a prediction.\n",
    "\n",
    "For `--scenario` set the task scenario your prediction is for: for scenario 1 that is \"baseline\", for scenario 2 and 3 it is \"perspective\".\n",
    "\n",
    "For `--prediction` you might have to include a subfolder in the path.\n",
    "\n",
    "If you want to evaluate a prediction for scenario 3, you will need to set `--implicit` as `True`, else you will need to set it as `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aa4e3b-0747-4856-a117-200133bca7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for baseline_method in [\"logreg_baseline\", \"rerank_baseline\"]:\n",
    "    for split in [\"dev\"]:\n",
    "        command = f\"py ./scripts/evaluation.py --data ./data-release --scenario baseline --split {split} --predictions  predictions/{baseline_method}_{split}_predictions.jsonl --output_dir results/{baseline_method} --diversity True --recall True --implicit False\"\n",
    "        os.system(f\"{command}\")\n",
    "\n",
    "        output = os.popen(command).read()\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb820ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
